export const blogPosts = [
    {
        slug: 'understanding-transformer-architectures',
        title: 'A Deep Dive into Transformer Architectures',
        excerpt: 'Explore the revolutionary architecture that powers modern NLP models like GPT and BERT. We break down self-attention, positional encodings, and the encoder-decoder structure.',
        thumbnail_url: '/media/projects/transformer-blog.jpg',
        created_date: '2024-03-10',
        published: true,
        tags: 'NLP, Deep Learning, Transformers',
        content: `
### The Revolution of Attention

Before transformers, recurrent neural networks (RNNs) and LSTMs were the state-of-the-art for sequence-to-sequence tasks. However, they struggled with long-range dependencies and lacked parallelization capabilities.

The 2017 paper "Attention Is All You Need" introduced the Transformer architecture, which completely abandoned recurrence and convolutions in favor of a mechanism called **self-attention**.

### Core Components

1.  **Self-Attention**: This is the heart of the transformer. For each token in a sequence, self-attention allows the model to weigh the importance of all other tokens in the same sequence. It helps the model understand context, like how "bank" means something different in "river bank" versus "investment bank".

2.  **Positional Encodings**: Since there's no recurrence, the model has no inherent sense of word order. Positional encodings are vectors added to the input embeddings to give the model information about the position of each token in the sequence.

3.  **Encoder-Decoder Stacks**:
    *   The **Encoder** stack processes the input sequence and builds a rich contextual representation.
    *   The **Decoder** stack takes the encoder's output and generates the output sequence, one token at a time, using the context provided.

This architecture has become the foundation for nearly all modern large language models (LLMs).
        `
    },
    {
        slug: 'building-ml-pipelines-with-mlflow',
        title: 'Building Reproducible ML Pipelines with MLflow',
        excerpt: 'Move beyond messy Jupyter notebooks. Learn how to structure a machine learning project with MLflow for experiment tracking, model packaging, and reproducibility.',
        thumbnail_url: '/media/projects/mlflow-blog.jpg',
        created_date: '2024-02-18',
        published: true,
        tags: 'MLOps, Data Science, Python',
        content: `
### The Problem with Notebooks

Jupyter notebooks are fantastic for exploration, but they can quickly become a nightmare for production ML. It's hard to track which parameters produced which model, and deploying a notebook is not straightforward.

### Enter MLflow

MLflow is an open-source platform to manage the ML lifecycle, including experimentation, reproducibility, and deployment. It has three primary components:

1.  **MLflow Tracking**: An API and UI for logging parameters, code versions, metrics, and artifacts when running your machine learning code. This creates a detailed record of every experiment.

2.  **MLflow Projects**: A standard format for packaging reusable data science code. You can define dependencies and entry points, ensuring your code runs the same way everywhere.

3.  **MLflow Models**: A convention for packaging machine learning models that can be used in a variety of downstream toolsâ€”for example, real-time serving through a REST API or batch inference on Apache Spark.

By integrating MLflow into your workflow, you create a system that is not only organized and trackable but also far easier to move from research to production.
        `
    }];
